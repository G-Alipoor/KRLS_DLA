function P = sKDLUpdate(X, P, varargin)

%
% sKDLUpdate
% This function updates the profile of the dictionary according to the online
% kernel dictionary learningalgorithm, proposed in the following paper:
%     C. O’Brien and M. D. Plumbley, "Sparse kernel dictionary learning," 2016
%     Proceedings of the 11th IMA International Conference on Mathematics in Signal Processing.
%
% Generated by Ghasem Alipoor (alipoor@hut.ac.ir) and Karl Skretting (karl.skretting@uis.no)
% Last modification: 13 June 2023
%

% In the refered paper, w variable is denoted as z.

pnames = {'GrowingTrsh',...% Growing threshold
    'maxL', ...                       % max value for profile L, ex 400
    'newL', ...                       % Profile L after pruning, ex 300
    'LearningRate_Rep', ...    % Learning Rate used in the Iterative Shrinkage-Thresholding Algorithm (ISTA)
    'LearningRate_Dic', ...     % Learning Rate used in the ISTA
    'ISTA_Thrsh', ...               % Convergence threshold used in the ISTA
    'lam_Rep', ...                   % Sparsity parameter for the representation coefficients term, lamda_1 in the paper
    'lam_Dic', ...                    % Sparsity parameter for the dictionary sparsity term, lamda_2 in the paper
    };
dflts = {.9999, 200, 190, 1e-7, 1e-13, ... % GrowingTrsh, maxL, newL, LearningRate_Rep, LearningRate_Dic
    0.01, 0.01, 0.01}; % ISTA_Thrsh, lam_Rep, lam_Dic
[GrowingTrsh, maxL, newL, LearningRate_Rep, LearningRate_Dic, ISTA_Thrsh, lam_Rep, lam_Dic] = ...
    getarg(varargin,pnames,dflts);

[N, M] = size(X);   % Input data size
L = size(P.X, 2);   % Profile size
Q = size(P.W, 1); % Number of atoms
if size(P.X, 1) ~= N
    error('Dimension of X_i is not proper.')
end
if size(P.W, 2) ~= L
    error('Dimension of W_i is not proper.')
end
if (size(P.K, 1) ~= L) || (size(P.K, 2) ~= L)
    error('Dimension of K_i is not proper.')
end
if (size(P.C, 1) ~= L) || (size(P.C, 2) ~= Q)
    error('Dimension of C_i is not proper.')
end

% Main iteration
for i = 1 : M
    % Step 1: get new signal x and calculate k and h vectors and sigma_2
    x = X(:,i);
    k = ker_eval(P.X, x, P.KernelType, P.KernelParam);                 % k(j) = <phi(X_i(:,j)),phi(x)>
    sigma_2 = ker_eval(x, x, P.KernelType, P.KernelParam);         % sigma_2 = K(x,x) = <phi(x),phi(x)>

    % Step 2: Check for the informativeness (the growing criterion in profile abstraction) of the new sample
    GrowingCriterionMeasure = k'*(P.K\k)/sigma_2;
    %     [~, r2] = myKOMP(P.C'*P.K*P.C, P.C'*k, sigma_2, 5, 1e-6, false, false);
    %     GrowingCriterionMeasure = 1 - sqrt(r2/sigma_2);
    %     GrowingCriterionMeasure = max(abs(k)./sqrt(sigma_2*diag(P.K)));
    if GrowingCriterionMeasure < GrowingTrsh
        % Update profile, when a new data sample is included.
        % The new input sample is included in the profile if GrowingCriterionMeasure is
        % smaller than a pre-specified threshold, GrowingTrsh. If x passes this critrion,
        % continue with the next steps, otherwise ignore x and get the next sample.
        L = L + 1;

        % Step 3: Sparse representation
        w = kISTA(P.C, k, P.K, lam_Rep, ISTA_Thrsh, LearningRate_Rep);

        P.X = cat(2, P.X, x);
        P.W = cat(2, P.W , w);
        P.K = cat(1, cat(2, P.K, k), cat(2, k', sigma_2));
        P.C = cat(1, P.C, zeros(1, Q));

        % Step 4: Compute (Update) A and B
        A = [k; sigma_2]*w';%cat(1, P.A, zeros(1, Q)) + [k; sigma_2]*w';
        B = w*w';%P.B + w*w';

        % Step 5: Update C
        P.C = UpdateC(P.C, P.K, A, B, lam_Dic, ISTA_Thrsh, LearningRate_Dic);
    end
end

% Extra step: Profile abstraction by discarding a sample data, when the profile grows
%     beyond an acceptble size, i.e. MaxProfileSize
%     This exculsion also have effect on some other matrices and variables.
if L > maxL
    % Step 5a: Finding the entry to be discarded
    % Indices of the data sample in reverse order of the cnontribution
    [~,m] = mink(diag(P.W'*P.W), L - newL);

    % Step 5b: updating the corresponding matrices and variables
    P.X(:, m)  = [];
    P.W(:, m) = [];
    P.K(:,m)   = [];
    P.K(m,:)   = [];
    P.C(m, :)  = [];
    %     P.A(m, :)  = [];
end
end

function W = kISTA(C, k, K, lam, Thr, LearningRate)
L = size(k, 2);
Q = size(C, 2);
W = zeros(Q, L);
for l = 1:L
    w_diff = inf;
    w_old = randn(Q, 1);
    iter = 1;
    while (w_diff > Thr) && (iter <= 200)
        w_new = Proximity(w_old + LearningRate*C'*(k(:, l) - K*C*w_old), lam*norm(w_old));
        w_diff = norm(w_new - w_old)/norm(w_old);
        w_old = w_new;
        iter = iter + 1;
    end
    W(:, l) = w_new;
end
end

function C = UpdateC(C, K, A, B, lam, Thr, LearningRate)
Q = size(C, 2);
etha = 1/size(C, 1);
for j = 1:Q
    c_q = C(:, j);
    c_diff = inf;
    iter = 0;
    while (c_diff > Thr) && (iter <= 200)
        c_new = Proximity(c_q + LearningRate*etha*(A(:, j) - K*c_q*B(j, j)), lam*norm(c_q));
        c_diff = norm(c_new - c_q)/norm(c_q);
        c_q = c_new;
        iter = iter + 1;
    end
    c_q = c_q/sqrt(c_q'*K*c_q + eps);
    C(:, j) = c_q;
end
end

function y = Proximity(x, lam)
y = sign(x).*max(0, abs(x) - lam);
end
